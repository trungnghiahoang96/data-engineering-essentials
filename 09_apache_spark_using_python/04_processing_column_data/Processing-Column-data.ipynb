{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56ae1ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 1.0.2\n"
     ]
    }
   ],
   "source": [
    "%load_ext nb_mypy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "849cf704",
   "metadata": {},
   "outputs": [],
   "source": [
    "%nb_mypy On"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c31e9821",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame, Window\n",
    "import pyspark.sql.functions as F\n",
    "from datetime import datetime\n",
    "import pathlib\n",
    "from __future__ import annotations\n",
    "from typing import Union, List, Tuple, Optional, TypeVar, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8caf2ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from os import path\n",
    "\n",
    "\n",
    "def read_hard_drive_data(spark: SparkSession, data_location: str, schema: str) -> DataFrame:\n",
    "    df = spark.read.csv(data_location, header='false', schema=schema)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99287170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_frame(spark: SparkSession, data: List[Tuple[Any, ...]], schema: str) -> DataFrame:\n",
    "    df = spark.createDataFrame(data, schema)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab5e94a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "def dateformat(df: DataFrame) -> DataFrame:\n",
    "    df = df.select('*', date_format('order_date', 'yyyyMM').alias('order_month'))\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c35623",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfd90baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "username\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '4000'). \\\n",
    "    config(\"spark.sql.warehouse.dir\", \"hdfs://0.0.0.0:9000/user/hive/warehouse/\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f'{username} | Python - Data Processing - Overview'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "120b48ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.11:4000\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>nghiaht7 | Python - Data Processing - Overview</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f0cac1a5a00>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c2d283f",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_path = '/home/nghiaht7/data-engineer/data-engineering-essentials/retail_db/orders'\n",
    "orders_schema = 'order_id INT, order_date STRING, order_customer_id INT, order_status STRING'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34e88b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = read_hard_drive_data(spark, orders_path, orders_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2e21525",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:...|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|            11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:...|             7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:...|             4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:...|             2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:...|             5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:...|             5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:...|              918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:...|             1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:...|             9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:...|             9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:...|             2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:...|             7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:...|             2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:...|             1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:...|             9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:...|             9198|     PROCESSING|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "orders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4ca1049",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:===============================================>        (64 + 2) / 75]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|order_month|count|\n",
      "+-----------+-----+\n",
      "|     201401| 5908|\n",
      "|     201405| 5467|\n",
      "|     201312| 5892|\n",
      "|     201310| 5335|\n",
      "|     201311| 6381|\n",
      "|     201307| 1533|\n",
      "|     201407| 4468|\n",
      "|     201403| 5778|\n",
      "|     201404| 5657|\n",
      "|     201402| 5635|\n",
      "|     201309| 5841|\n",
      "|     201406| 5308|\n",
      "|     201308| 5680|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "orders. \\\n",
    "    groupBy(date_format('order_date', 'yyyyMM').alias('order_month')). \\\n",
    "    count(). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0f4e623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+-----------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|order_month|\n",
      "+--------+--------------------+-----------------+---------------+-----------+\n",
      "|       1|2013-07-25 00:00:...|            11599|         CLOSED|     201307|\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|     201307|\n",
      "|       3|2013-07-25 00:00:...|            12111|       COMPLETE|     201307|\n",
      "|       4|2013-07-25 00:00:...|             8827|         CLOSED|     201307|\n",
      "|       5|2013-07-25 00:00:...|            11318|       COMPLETE|     201307|\n",
      "|       6|2013-07-25 00:00:...|             7130|       COMPLETE|     201307|\n",
      "|       7|2013-07-25 00:00:...|             4530|       COMPLETE|     201307|\n",
      "|       8|2013-07-25 00:00:...|             2911|     PROCESSING|     201307|\n",
      "|       9|2013-07-25 00:00:...|             5657|PENDING_PAYMENT|     201307|\n",
      "|      10|2013-07-25 00:00:...|             5648|PENDING_PAYMENT|     201307|\n",
      "|      11|2013-07-25 00:00:...|              918| PAYMENT_REVIEW|     201307|\n",
      "|      12|2013-07-25 00:00:...|             1837|         CLOSED|     201307|\n",
      "|      13|2013-07-25 00:00:...|             9149|PENDING_PAYMENT|     201307|\n",
      "|      14|2013-07-25 00:00:...|             9842|     PROCESSING|     201307|\n",
      "|      15|2013-07-25 00:00:...|             2568|       COMPLETE|     201307|\n",
      "|      16|2013-07-25 00:00:...|             7276|PENDING_PAYMENT|     201307|\n",
      "|      17|2013-07-25 00:00:...|             2667|       COMPLETE|     201307|\n",
      "|      18|2013-07-25 00:00:...|             1205|         CLOSED|     201307|\n",
      "|      19|2013-07-25 00:00:...|             9488|PENDING_PAYMENT|     201307|\n",
      "|      20|2013-07-25 00:00:...|             9198|     PROCESSING|     201307|\n",
      "+--------+--------------------+-----------------+---------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders: DataFrame = dateformat(orders)\n",
    "orders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d1a05c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "387ed7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees: List[Tuple[Any, ...]] = [\n",
    "    (1, \"Scott\", \"Tiger\", 1000.0, \n",
    "      \"united states\", \"+1 123 456 7890\", \"123 45 6789\"\n",
    "    ),\n",
    "     (2, \"Henry\", \"Ford\", 1250.0, \n",
    "      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n",
    "     ),\n",
    "     (3, \"Nick\", \"Junior\", 750.0, \n",
    "      \"united KINGDOM\", \"+44 111 111 1111\", \"222 33 4444\"\n",
    "     ),\n",
    "     (4, \"Bill\", \"Gomes\", 1500.0, \n",
    "      \"AUSTRALIA\", \"+61 987 654 3210\", \"789 12 6118\"\n",
    "     )\n",
    "]\n",
    "\n",
    "employees_schema:str = \"\"\"employee_id INT, first_name STRING, \n",
    "                    last_name STRING, salary FLOAT, nationality STRING,\n",
    "                    phone_number STRING, ssn STRING\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24fdd37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- salary: float (nullable = true)\n",
      " |-- nationality: string (nullable = true)\n",
      " |-- phone_number: string (nullable = true)\n",
      " |-- ssn: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF = create_data_frame(spark, employees, employees_schema)\n",
    "employeesDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65c87b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 12:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|     Scott|    Tiger|\n",
      "|     Henry|     Ford|\n",
      "|      Nick|   Junior|\n",
      "|      Bill|    Gomes|\n",
      "+----------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 13:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    select(\"first_name\", \"last_name\"). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a16adf5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+---------+\n",
      "|employee_id|first_name_upper|last_name|\n",
      "+-----------+----------------+---------+\n",
      "|          4|            BILL|    Gomes|\n",
      "|          3|            NICK|   Junior|\n",
      "|          2|           HENRY|     Ford|\n",
      "|          1|           SCOTT|    Tiger|\n",
      "+-----------+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, asc, desc, upper\n",
    "\n",
    "employeesDF. \\\n",
    "    select(col(\"employee_id\"),upper(col(\"first_name\")).alias(\"first_name_upper\"), col(\"last_name\")). \\\n",
    "    orderBy(col(\"employee_id\").desc()). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69815b47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb24fe64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|full_name   |\n",
      "+------------+\n",
      "|Scott, Tiger|\n",
      "|Henry, Ford |\n",
      "|Nick, Junior|\n",
      "|Bill, Gomes |\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat, col, lit\n",
    "\n",
    "employeesDF. \\\n",
    "    select(concat(col(\"first_name\"), \n",
    "                  lit(\", \"), \n",
    "                  col(\"last_name\")\n",
    "                 ).alias(\"full_name\")\n",
    "          ). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad00d874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+-----------+-----------+---------+\n",
      "|employee_id|    phone_number|        ssn|phone_last4|ssn_last4|\n",
      "+-----------+----------------+-----------+-----------+---------+\n",
      "|          1| +1 123 456 7890|123 45 6789|       7890|     6789|\n",
      "|          2|+91 234 567 8901|456 78 9123|       8901|     9123|\n",
      "|          3|+44 111 111 1111|222 33 4444|       1111|     4444|\n",
      "|          4|+61 987 654 3210|789 12 6118|       3210|     6118|\n",
      "+-----------+----------------+-----------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import substring, col\n",
    "\n",
    "employeesDF. \\\n",
    "    select(\"employee_id\", \"phone_number\", \"ssn\"). \\\n",
    "    withColumn(\"phone_last4\", substring(col(\"phone_number\"), -4, 4).cast(\"int\")). \\\n",
    "    withColumn(\"ssn_last4\", substring(col(\"ssn\"), 8, 4).cast(\"int\")). \\\n",
    "    show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0fcbe6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56c27786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------+\n",
      "|employee                                                                      |\n",
      "+------------------------------------------------------------------------------+\n",
      "|00001Scott-----Tiger-----00001000.0united states--+1 123 456 7890--123 45 6789|\n",
      "|00002Henry-----Ford------00001250.0India----------+91 234 567 8901-456 78 9123|\n",
      "|00003Nick------Junior----00000750.0united KINGDOM-+44 111 111 1111-222 33 4444|\n",
      "|00004Bill------Gomes-----00001500.0AUSTRALIA------+61 987 654 3210-789 12 6118|\n",
      "+------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#padding string\n",
    "\n",
    "from pyspark.sql.functions import lpad, rpad, concat\n",
    "\n",
    "empFixedDF = employeesDF.select(\n",
    "    concat(\n",
    "        lpad(\"employee_id\", 5, \"0\"), \n",
    "        rpad(\"first_name\", 10, \"-\"), \n",
    "        rpad(\"last_name\", 10, \"-\"),\n",
    "        lpad(\"salary\", 10, \"0\"), \n",
    "        rpad(\"nationality\", 15, \"-\"), \n",
    "        rpad(\"phone_number\", 17, \"-\"), \n",
    "        \"ssn\"\n",
    "    ).alias(\"employee\")\n",
    ")\n",
    "empFixedDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1631ad94",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimes: List[Tuple[Any, ...]] = [(\"2014-02-28\", \"2014-02-28 10:00:00.123\"),\n",
    "                     (\"2016-02-29\", \"2016-02-29 08:08:08.999\"),\n",
    "                     (\"2017-10-31\", \"2017-12-31 11:59:59.123\"),\n",
    "                     (\"2019-11-30\", \"2019-08-31 00:00:00.000\")\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72f7243c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+\n",
      "|date      |time                   |\n",
      "+----------+-----------------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|\n",
      "+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF = create_data_frame(spark, datetimes, schema=\"date STRING, time STRING\")\n",
    "\n",
    "datetimesDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2427aeda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------+-------------+-------------+-------------+\n",
      "|      date|                time|date_add_date|date_add_time|date_sub_date|date_sub_time|\n",
      "+----------+--------------------+-------------+-------------+-------------+-------------+\n",
      "|2014-02-28|2014-02-28 10:00:...|   2014-03-10|   2014-03-10|   2014-02-18|   2014-02-18|\n",
      "|2016-02-29|2016-02-29 08:08:...|   2016-03-10|   2016-03-10|   2016-02-19|   2016-02-19|\n",
      "|2017-10-31|2017-12-31 11:59:...|   2017-11-10|   2018-01-10|   2017-10-21|   2017-12-21|\n",
      "|2019-11-30|2019-08-31 00:00:...|   2019-12-10|   2019-09-10|   2019-11-20|   2019-08-21|\n",
      "+----------+--------------------+-------------+-------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# adding 10 days to date, time\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "datetimesDF. \\\n",
    "    withColumn(\"date_add_date\", F.date_add(\"date\", 10)). \\\n",
    "    withColumn(\"date_add_time\", F.date_add(\"time\", 10)). \\\n",
    "    withColumn(\"date_sub_date\", F.date_sub(\"date\", 10)). \\\n",
    "    withColumn(\"date_sub_time\", F.date_sub(\"time\", 10)). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "608ecf57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+----------+----------+\n",
      "|date      |time                   |date_trunc|time_trunc|\n",
      "+----------+-----------------------+----------+----------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|2014-02-01|2014-01-01|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|2016-02-01|2016-01-01|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|2017-10-01|2017-01-01|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|2019-11-01|2019-01-01|\n",
      "+----------+-----------------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF. \\\n",
    "    withColumn(\"date_trunc\", F.trunc(\"date\", \"MM\")). \\\n",
    "    withColumn(\"time_trunc\", F.trunc(\"time\", \"yy\")). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2f27c280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+-----+----------+---------+----------+---------+\n",
      "|current_date|year|month|weekofyear|dayofyear|dayofmonth|dayofweek|\n",
      "+------------+----+-----+----------+---------+----------+---------+\n",
      "|  2021-08-25|2021|    8|        34|      237|        25|        4|\n",
      "+------------+----+-----+----------+---------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, month, weekofyear, dayofmonth, \\\n",
    "    dayofyear, dayofweek, current_date\n",
    "\n",
    "df = spark.createDataFrame([(\"X\", )]).toDF(\"dummy\")\n",
    "\n",
    "df.select(\n",
    "    current_date().alias('current_date'), \n",
    "    year(current_date()).alias('year'),\n",
    "    month(current_date()).alias('month'),\n",
    "    weekofyear(current_date()).alias('weekofyear'),\n",
    "    dayofyear(current_date()).alias('dayofyear'),\n",
    "    dayofmonth(current_date()).alias('dayofmonth'),\n",
    "    dayofweek(current_date()).alias('dayofweek')\n",
    ").show() #yyyy-MM-dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0e6f5e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+----+-----+----------+----+------+------+\n",
      "|current_timestamp     |year|month|dayofmonth|hour|minute|second|\n",
      "+----------------------+----+-----+----------+----+------+------+\n",
      "|2021-08-25 21:04:16.46|2021|8    |25        |21  |4     |16    |\n",
      "+----------------------+----+-----+----------+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_timestamp, hour, minute, second\n",
    "\n",
    "df.select(\n",
    "    current_timestamp().alias('current_timestamp'), \n",
    "    year(current_timestamp()).alias('year'),\n",
    "    month(current_timestamp()).alias('month'),\n",
    "    dayofmonth(current_timestamp()).alias('dayofmonth'),\n",
    "    hour(current_timestamp()).alias('hour'),\n",
    "    minute(current_timestamp()).alias('minute'),\n",
    "    second(current_timestamp()).alias('second')\n",
    ").show(truncate=False) #yyyy-MM-dd HH:mm:ss.SSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2ffe9c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees = [(1, \"Scott\", \"Tiger\", 1000.0, 10,\n",
    "                      \"united states\", \"+1 123 456 7890\", \"123 45 6789\"\n",
    "                     ),\n",
    "                     (2, \"Henry\", \"Ford\", 1250.0, None,\n",
    "                      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n",
    "                     ),\n",
    "                     (3, \"Nick\", \"Junior\", 750.0, '',\n",
    "                      \"united KINGDOM\", \"+44 111 111 1111\", \"222 33 4444\"\n",
    "                     ),\n",
    "                     (4, \"Bill\", \"Gomes\", 1500.0, 10,\n",
    "                      \"AUSTRALIA\", \"+61 987 654 3210\", \"789 12 6118\"\n",
    "                     )\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "58eabb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "employees = create_data_frame(spark, employees, schema=\"\"\"employee_id INT, first_name STRING, \n",
    "                    last_name STRING, salary FLOAT, bonus STRING, nationality STRING,\n",
    "                    phone_number STRING, ssn STRING\"\"\")\n",
    "\n",
    "employees.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "30aae79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+--------------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|bonus_filled_0|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+--------------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|            10|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|             0|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|             0|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|            10|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, coalesce\n",
    "\n",
    "employees. \\\n",
    "    withColumn('bonus_filled_0', coalesce(col('bonus').cast('int'), lit(0))). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3345f6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+---------------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|bonus_case_when|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+---------------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|             10|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|              0|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|              0|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|             10|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "employees. \\\n",
    "    withColumn(\n",
    "        'bonus_case_when', \n",
    "        F.expr(\"\"\"\n",
    "            CASE WHEN bonus IS NULL OR bonus = '' THEN 0\n",
    "            ELSE bonus\n",
    "            END\n",
    "            \"\"\")\n",
    "    ). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17d2995",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
