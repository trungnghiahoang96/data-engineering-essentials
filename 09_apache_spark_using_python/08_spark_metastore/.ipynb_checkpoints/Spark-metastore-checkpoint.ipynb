{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b12fa63",
   "metadata": {},
   "source": [
    "## Overview of Spark Metastore\n",
    "\n",
    "Let us get an overview of Spark Metastore and how we can leverage it to manage databases and tables on top of Big Data based file systems such as HDFS, s3 etc.\n",
    "\n",
    "* Quite often we need to deal with structured data and the most popular way of processing structured data is by using Databases, Tables and then SQL.\n",
    "* Spark Metastore (similar to Hive Metastore) will facilitate us to manage databases and tables.\n",
    "* Typically Metastore is setup using traditional relational database technologies such as **Oracle**, **MySQL**, **Postgres** etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ceb2c9",
   "metadata": {},
   "source": [
    "## Exploring Spark Catalog\n",
    "\n",
    "Let us get an overview of Spark Catalog to manage Spark Metastore tables as well as temporary views. \n",
    "* Let us say `spark` is of type `SparkSession`. There is an attribute as part of `spark` called as catalog and it is of type pyspark.sql.catalog.Catalog.\n",
    "* We can access catalog using `spark.catalog`.\n",
    "* We can permanently or temporarily create tables or views on top of data in a Data Frame.\n",
    "* Metadata such as table names, column names, data types etc for the permanent tables or views will be stored in Metastore. We can access the metadata using `spark.catalog` which is exposed as part of SparkSession object.\n",
    "* `spark.catalog` also provide us the details related to temporary views that are being created. Metadata of these temporary views will not be stored in Spark Metastore.\n",
    "* Permanent tables are typically created using databases in spark metastore. If not specified, the tables will be created in **default** database.\n",
    "* There are several methods that are part of `spark.catalog`. We will explore them in the later topics.\n",
    "* Following are some of the tasks that can be performed using `spark.catalog` object.\n",
    "  * Check current database and switch to different databases.\n",
    "  * Create permanent table in metastore.\n",
    "  * Create or drop temporary views.\n",
    "  * Register functions.\n",
    "* All the above tasks can be performed using SQL style commands passed to `spark.sql`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fc7b751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    config(\"spark.sql.debug.maxToStringFields\", 1000). \\\n",
    "    config(\"spark.dynamicAllocation.enabled\", \"false\"). \\\n",
    "    config(\"spark.sql.catalogImplementation\",\"hive\"). \\\n",
    "    config(\"spark.sql.warehouse.dir\", \"hdfs://0.0.0.0:9000/user/hive/warehouse/\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f'{username} | Python - Spark Metastore'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()\n",
    "\n",
    "# config(\"hive.metastore.uris\", \"jdbc:postgresql://localhost:6432/metastore\"). \\\n",
    "# (\"hive.metastore.uris\", \"thrift://METASTORE:9083\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aba4ac30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.catalog.Catalog at 0x7f8cd8380f40>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3ebd5c",
   "metadata": {},
   "source": [
    "## Creating Metastore Tables using catalog\n",
    "\n",
    "Data Frames can be written into Metastore Tables using APIs such as `saveAsTable` and `insertInto` available as part of write on top of objects of type Data Frame.\n",
    "\n",
    "* We can create a new table using Data Frame using `saveAsTable`. We can also create an empty table by using `spark.catalog.createTable` or `spark.catalog.createExternalTable`.\n",
    "* We can also prefix the database name to write data into tables belonging to a particular database. If the database is not specified then the session will be attached to default database.\n",
    "* We can also attach or connect the current session to a specific database using `spark.catalog.setCurrentDatabase`.\n",
    "* Databases can be created using `spark.sql(\"CREATE DATABASE database_name\")`. We can list Databases using `spark.sql` or `spark.catalog.listDatabases()`\n",
    "* We can use modes such as `append`, `overwrite` and `error` with `saveAsTable`. Default is error.\n",
    "* We can use modes such as `append` and `overwrite` with `insertInto`. Default is append.\n",
    "* When we use `saveAsTable`, following happens:\n",
    "  * Check for table if the table already exists. By default `saveAsTable` will throw exception.\n",
    "  * If the table does not exists the table will be created.\n",
    "  * Data from Data Frame will be copied into the table.\n",
    "  * We can alter the behavior by using mode. We can overwrite the existing table or we can append into it.\n",
    "* We can list the tables using `spark.catalog.listTables` after switching to appropriate database using `spark.catalog.setCurrentDatabase`.\n",
    "* We can also switch the database and list tables using `spark.sql`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1abf65bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='Default Hive database', locationUri='hdfs://0.0.0.0:9000/user/hive/warehouse'),\n",
       " Database(name='retail_db', description='', locationUri='hdfs://0.0.0.0:9000/user/hive/warehouse/retail_db.db')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80e1e90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/30 11:27:53 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DROP DATABASE IF EXISTS {username}_demo_db CASCADE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "257ea469",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/30 11:34:54 WARN ObjectStore: Failed to get database nghiaht7_demo_db, returning NoSuchObjectException\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"CREATE DATABASE {username}_demo_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "039a67f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='Default Hive database', locationUri='hdfs://0.0.0.0:9000/user/hive/warehouse'),\n",
       " Database(name='nghiaht7_demo_db', description='', locationUri='hdfs://0.0.0.0:9000/user/hive/warehouse/nghiaht7_demo_db.db'),\n",
       " Database(name='retail_db', description='', locationUri='hdfs://0.0.0.0:9000/user/hive/warehouse/retail_db.db')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e24d7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentDatabase(f'{username}_demo_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77b23cb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nghiaht7_demo_db'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2de47dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [(\"X\", )]\n",
    "df = spark.createDataFrame(l, schema=\"dummy STRING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b2ee4f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb941507",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|dummy|\n",
      "+-----+\n",
      "|    X|\n",
      "+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62df14e",
   "metadata": {},
   "source": [
    "use spark context create dataframe --> write to spark warehouse --> so can manage by spark.catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7fead1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.saveAsTable(\"dual\", mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3444b8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='dual', database='nghiaht7_demo_db', description=None, tableType='MANAGED', isTemporary=False)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f7e54e",
   "metadata": {},
   "source": [
    "Can query using SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc8f2722",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>dummy</th></tr>\n",
       "<tr><td>X</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----+\n",
       "|dummy|\n",
       "+-----+\n",
       "|    X|\n",
       "+-----+"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM nghiaht7_demo_db.dual;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9bf03dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|dummy|\n",
      "+-----+\n",
      "|    X|\n",
      "+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"dual\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0d4c8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ee6da5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 18:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|dummy|\n",
      "+-----+\n",
      "|    X|\n",
      "+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# direct query because current database is nghiaht7_demo_db\n",
    "\n",
    "spark.sql('SELECT * FROM dual').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34fac60f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE dual\")\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "96ae4878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(dummy,StringType,true)))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715564ad",
   "metadata": {},
   "source": [
    "## Create and drop Table using Spark.Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b24d798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bad474cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>dummy</th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----+\n",
       "|dummy|\n",
       "+-----+\n",
       "+-----+"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.createTable('dual2', schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "48b2e903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='dual', database='nghiaht7_demo_db', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='dual2', database='nghiaht7_demo_db', description=None, tableType='MANAGED', isTemporary=False)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "94ede5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.insertInto?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d2384a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.insertInto('dual2')\n",
    "df.write.insertInto('dual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4ec38cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|dummy|\n",
      "+-----+\n",
      "|    X|\n",
      "|    X|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"dual\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1494d8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|dummy|\n",
      "+-----+\n",
      "|    X|\n",
      "|    X|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM dual2').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7625a2af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "73278581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE dual\")\n",
    "spark.sql(\"DROP TABLE dual2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a7c2baac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DROP DATABASE {username}_demo_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d0eedf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/30 20:22:43 WARN ObjectStore: Failed to get database nghiaht7_demo_db, returning NoSuchObjectException\n",
      "21/08/30 20:22:43 WARN ObjectStore: Failed to get database nghiaht7_demo_db, returning NoSuchObjectException\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can use CASCADE to drop database along with tables.\n",
    "spark.sql(f\"DROP DATABASE IF EXISTS {username}_demo_db CASCADE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cf6b50",
   "metadata": {},
   "source": [
    "## Inferring Schema for Tables\n",
    "\n",
    "When we want to create a table using `spark.catalog.createTable` or using `spark.catalog.createExternalTable`, we need to specify Schema.\n",
    "\n",
    "* Schema can be inferred from the Dataframe and then can be passed using `StructType` object while creating the table.\n",
    "* `StructType` takes list of objects of type `StructField`.\n",
    "* `StructField` is built using column name and data type. All the data types are available under `pyspark.sql.types`.\n",
    "* We need to pass table name and schema for `spark.catalog.createTable`.\n",
    "* We have to pass path along with name and schema for `spark.catalog.createExternalTable`.\n",
    "* We can use source to define file format along with applicable options. For example, if we want to create a table for CSV, then source will be csv and we can pass applicable options for CSV such as sep, header etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c15cf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.createExternalTable?\n",
    "\n",
    "# Signature:\n",
    "# spark.catalog.createExternalTable(\n",
    "#     tableName,\n",
    "#     path=None,\n",
    "#     source=None,\n",
    "#     schema=None,\n",
    "#     **options,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "001d50c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/30 20:24:01 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "21/08/30 20:24:01 WARN ObjectStore: Failed to get database nghiaht7_airtraffic, returning NoSuchObjectException\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {username}_airtraffic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76892910",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentDatabase(f\"{username}_airtraffic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfc74356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nghiaht7_airtraffic'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f196c4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -put /home/nghiaht7/data-engineer/data-engineering-essentials/data/airport-codes.csv /user/nghiaht7/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a47a85dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   1 nghiaht7 hadoop       6791 2021-08-30 20:33 /user/nghiaht7/airport-codes.csv\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls -R /user | grep airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76b9a060",
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_codes_path = \"hdfs://0.0.0.0:9000/user/nghiaht7/airport-codes.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad615b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('DROP TABLE IF EXISTS airport_codes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7503c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/30 20:34:59 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider csv. Persisting data source table `nghiaht7_airtraffic`.`airport_codes` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>IATA_CODE</th><th>CITY</th><th>STATE</th><th>COUNTRY</th></tr>\n",
       "<tr><td>ABE</td><td>Allentown</td><td>PA</td><td>USA</td></tr>\n",
       "<tr><td>ABI</td><td>Abilene</td><td>TX</td><td>USA</td></tr>\n",
       "<tr><td>ABQ</td><td>Albuquerque</td><td>NM</td><td>USA</td></tr>\n",
       "<tr><td>ABR</td><td>Aberdeen</td><td>SD</td><td>USA</td></tr>\n",
       "<tr><td>ABY</td><td>Albany</td><td>GA</td><td>USA</td></tr>\n",
       "<tr><td>ACK</td><td>Nantucket</td><td>MA</td><td>USA</td></tr>\n",
       "<tr><td>ACT</td><td>Waco</td><td>TX</td><td>USA</td></tr>\n",
       "<tr><td>ACV</td><td>Arcata/Eureka</td><td>CA</td><td>USA</td></tr>\n",
       "<tr><td>ACY</td><td>Atlantic City</td><td>NJ</td><td>USA</td></tr>\n",
       "<tr><td>ADK</td><td>Adak</td><td>AK</td><td>USA</td></tr>\n",
       "<tr><td>ADQ</td><td>Kodiak</td><td>AK</td><td>USA</td></tr>\n",
       "<tr><td>AEX</td><td>Alexandria</td><td>LA</td><td>USA</td></tr>\n",
       "<tr><td>AGS</td><td>Augusta</td><td>GA</td><td>USA</td></tr>\n",
       "<tr><td>AKN</td><td>King Salmon</td><td>AK</td><td>USA</td></tr>\n",
       "<tr><td>ALB</td><td>Albany</td><td>NY</td><td>USA</td></tr>\n",
       "<tr><td>ALO</td><td>Waterloo</td><td>IA</td><td>USA</td></tr>\n",
       "<tr><td>AMA</td><td>Amarillo</td><td>TX</td><td>USA</td></tr>\n",
       "<tr><td>ANC</td><td>Anchorage</td><td>AK</td><td>USA</td></tr>\n",
       "<tr><td>APN</td><td>Alpena</td><td>MI</td><td>USA</td></tr>\n",
       "<tr><td>ASE</td><td>Aspen</td><td>CO</td><td>USA</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+---------+-------------+-----+-------+\n",
       "|IATA_CODE|         CITY|STATE|COUNTRY|\n",
       "+---------+-------------+-----+-------+\n",
       "|      ABE|    Allentown|   PA|    USA|\n",
       "|      ABI|      Abilene|   TX|    USA|\n",
       "|      ABQ|  Albuquerque|   NM|    USA|\n",
       "|      ABR|     Aberdeen|   SD|    USA|\n",
       "|      ABY|       Albany|   GA|    USA|\n",
       "|      ACK|    Nantucket|   MA|    USA|\n",
       "|      ACT|         Waco|   TX|    USA|\n",
       "|      ACV|Arcata/Eureka|   CA|    USA|\n",
       "|      ACY|Atlantic City|   NJ|    USA|\n",
       "|      ADK|         Adak|   AK|    USA|\n",
       "|      ADQ|       Kodiak|   AK|    USA|\n",
       "|      AEX|   Alexandria|   LA|    USA|\n",
       "|      AGS|      Augusta|   GA|    USA|\n",
       "|      AKN|  King Salmon|   AK|    USA|\n",
       "|      ALB|       Albany|   NY|    USA|\n",
       "|      ALO|     Waterloo|   IA|    USA|\n",
       "|      AMA|     Amarillo|   TX|    USA|\n",
       "|      ANC|    Anchorage|   AK|    USA|\n",
       "|      APN|       Alpena|   MI|    USA|\n",
       "|      ASE|        Aspen|   CO|    USA|\n",
       "+---------+-------------+-----+-------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tạo từ spark.catalog nên path cũng nên nằm trong phần hdfs://0.0.0.0:9000/user ...\n",
    "\n",
    "spark.catalog. \\\n",
    "    createExternalTable(\"airport_codes\",\n",
    "                        path=airport_codes_path,\n",
    "                        source=\"csv\",\n",
    "                        sep=\",\",\n",
    "                        header=\"true\",\n",
    "                        inferSchema=\"true\"\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02b29f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='airport_codes', database='nghiaht7_airtraffic', description=None, tableType='EXTERNAL', isTemporary=False)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a809d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 14:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+-----+-------+\n",
      "|IATA_CODE|         CITY|STATE|COUNTRY|\n",
      "+---------+-------------+-----+-------+\n",
      "|      ABE|    Allentown|   PA|    USA|\n",
      "|      ABI|      Abilene|   TX|    USA|\n",
      "|      ABQ|  Albuquerque|   NM|    USA|\n",
      "|      ABR|     Aberdeen|   SD|    USA|\n",
      "|      ABY|       Albany|   GA|    USA|\n",
      "|      ACK|    Nantucket|   MA|    USA|\n",
      "|      ACT|         Waco|   TX|    USA|\n",
      "|      ACV|Arcata/Eureka|   CA|    USA|\n",
      "|      ACY|Atlantic City|   NJ|    USA|\n",
      "|      ADK|         Adak|   AK|    USA|\n",
      "|      ADQ|       Kodiak|   AK|    USA|\n",
      "|      AEX|   Alexandria|   LA|    USA|\n",
      "|      AGS|      Augusta|   GA|    USA|\n",
      "|      AKN|  King Salmon|   AK|    USA|\n",
      "|      ALB|       Albany|   NY|    USA|\n",
      "|      ALO|     Waterloo|   IA|    USA|\n",
      "|      AMA|     Amarillo|   TX|    USA|\n",
      "|      ANC|    Anchorage|   AK|    USA|\n",
      "|      APN|       Alpena|   MI|    USA|\n",
      "|      ASE|        Aspen|   CO|    USA|\n",
      "+---------+-------------+-----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"airport_codes\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "54d2f1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+---------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                |comment|\n",
      "+----------------------------+---------------------------------------------------------+-------+\n",
      "|IATA_CODE                   |string                                                   |null   |\n",
      "|CITY                        |string                                                   |null   |\n",
      "|STATE                       |string                                                   |null   |\n",
      "|COUNTRY                     |string                                                   |null   |\n",
      "|                            |                                                         |       |\n",
      "|# Detailed Table Information|                                                         |       |\n",
      "|Database                    |nghiaht7_airtraffic                                      |       |\n",
      "|Table                       |airport_codes                                            |       |\n",
      "|Owner                       |nghiaht7                                                 |       |\n",
      "|Created Time                |Mon Aug 30 20:35:09 ICT 2021                             |       |\n",
      "|Last Access                 |UNKNOWN                                                  |       |\n",
      "|Created By                  |Spark 3.1.2                                              |       |\n",
      "|Type                        |EXTERNAL                                                 |       |\n",
      "|Provider                    |csv                                                      |       |\n",
      "|Location                    |hdfs://0.0.0.0:9000/user/nghiaht7/airport-codes.csv      |       |\n",
      "|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe       |       |\n",
      "|InputFormat                 |org.apache.hadoop.mapred.SequenceFileInputFormat         |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat|       |\n",
      "|Storage Properties          |[inferSchema=true, sep=,, header=true]                   |       |\n",
      "+----------------------------+---------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('DESCRIBE FORMATTED airport_codes').show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2caf7cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column(name='IATA_CODE', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='CITY', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='STATE', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='COUNTRY', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listColumns('airport_codes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723d0b52",
   "metadata": {},
   "source": [
    "## Define Schema for Tables using StructType\n",
    "\n",
    "When we want to create a table using `spark.catalog.createTable` or using `spark.catalog.createExternalTable`, we need to specify Schema.\n",
    "\n",
    "* Schema can be inferred or we can pass schema using `StructType` object while creating the table..\n",
    "* `StructType` takes list of objects of type `StructField`.\n",
    "* `StructField` is built using column name and data type. All the data types are available under `pyspark.sql.types`.\n",
    "* We need to pass table name and schema for `spark.catalog.createTable`.\n",
    "* We have to pass path along with name and schema for `spark.catalog.createExternalTable`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "611ad1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, \\\n",
    "    IntegerType, StringType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f232eaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesSchema = StructType([\n",
    "    StructField(\"employee_id\", IntegerType()),\n",
    "    StructField(\"first_name\", StringType()),\n",
    "    StructField(\"last_name\", StringType()),\n",
    "    StructField(\"salary\", FloatType()),\n",
    "    StructField(\"nationality\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4af6f43e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(employee_id,IntegerType,true),StructField(first_name,StringType,true),StructField(last_name,StringType,true),StructField(salary,FloatType,true),StructField(nationality,StringType,true)))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employeesSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a35e83d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'struct<employee_id:int,first_name:string,last_name:string,salary:float,nationality:string>'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employeesSchema.simpleString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "db3be31d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('DROP TABLE IF EXISTS employees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1bc81551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>employee_id</th><th>first_name</th><th>last_name</th><th>salary</th><th>nationality</th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------+----------+---------+------+-----------+\n",
       "|employee_id|first_name|last_name|salary|nationality|\n",
       "+-----------+----------+---------+------+-----------+\n",
       "+-----------+----------+---------+------+-----------+"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.createTable(\"employees\", schema=employeesSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "edb591a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='airport_codes', database='nghiaht7_airtraffic', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='employees', database='nghiaht7_airtraffic', description=None, tableType='MANAGED', isTemporary=False)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "211b1f40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column(name='employee_id', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='first_name', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='last_name', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='salary', description=None, dataType='float', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='nationality', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listColumns('employees')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7793ff1",
   "metadata": {},
   "source": [
    "## Inserting into Existing Tables\n",
    "\n",
    "Let us understand how we can insert data into existing tables using `insertInto`.\n",
    "\n",
    "* We can use modes such as `append` and `overwrite` with `insertInto`. Default is `append`.\n",
    "* When we use `insertInto`, following happens:\n",
    "  * If the table does not exist, `insertInto` will throw an exception.\n",
    "  * If the table exists, by default data will be appended.\n",
    "  * We can alter the behavior by using keyword argument overwrite. It is by default False, we can pass True to replace existing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aeced6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees = [(1, \"Scott\", \"Tiger\", 1000.0, \"united states\"),\n",
    "             (2, \"Henry\", \"Ford\", 1250.0, \"India\"),\n",
    "             (3, \"Nick\", \"Junior\", 750.0, \"united KINGDOM\"),\n",
    "             (4, \"Bill\", \"Gomes\", 1500.0, \"AUSTRALIA\")\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "176bc76a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(employee_id,IntegerType,true),StructField(first_name,StringType,true),StructField(last_name,StringType,true),StructField(salary,FloatType,true),StructField(nationality,StringType,true)))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.table('employees').schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8d82f75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 21:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|\n",
      "+-----------+----------+---------+------+--------------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states|\n",
      "|          2|     Henry|     Ford|1250.0|         India|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|\n",
      "+-----------+----------+---------+------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "employeesDF = spark.createDataFrame(employees,\n",
    "    schema=\"\"\"employee_id INT, first_name STRING, last_name STRING,\n",
    "              salary FLOAT, nationality STRING\n",
    "           \"\"\"\n",
    ")\n",
    "\n",
    "employeesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5f3b12be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "employeesDF.write.insertInto(\"employees\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "de6b1fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 24:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|\n",
      "+-----------+----------+---------+------+--------------+\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|\n",
      "|          1|     Scott|    Tiger|1000.0| united states|\n",
      "|          2|     Henry|     Ford|1250.0|         India|\n",
      "+-----------+----------+---------+------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"employees\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eafd9bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 27:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|\n",
      "+-----------+----------+---------+------+--------------+\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|\n",
      "|          1|     Scott|    Tiger|1000.0| united states|\n",
      "|          2|     Henry|     Ford|1250.0|         India|\n",
      "+-----------+----------+---------+------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM employees').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d2a1ee",
   "metadata": {},
   "source": [
    "## Read and Process data from Metastore Tables\n",
    "\n",
    "Let us see how we can read tables using functions such as `spark.read.table` and process data using Data Frame APIs.\n",
    "\n",
    "* Using Data Frame APIs - `spark.read.table(\"table_name\")`.\n",
    "* We can also prefix the database name to read tables belonging to a particular database.\n",
    "* When we read the table, it will result in a Data Frame.\n",
    "* Once Data Frame is created we can use functions such as `filter` or `where`, `groupBy`, `sort` or `orderBy` to process the data in the Data Frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8402d60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "airport_codes_df = spark. \\\n",
    "    read. \\\n",
    "    csv(airport_codes_path,\n",
    "        sep=\",\",\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7247f02c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='airport_codes', database='nghiaht7_airtraffic', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='employees', database='nghiaht7_airtraffic', description=None, tableType='MANAGED', isTemporary=False)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f325a7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_codes_catalog = spark.read.table(\"airport_codes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a87bf072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(airport_codes_catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "999a3a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- IATA_CODE: string (nullable = true)\n",
      " |-- CITY: string (nullable = true)\n",
      " |-- STATE: string (nullable = true)\n",
      " |-- COUNTRY: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_codes_catalog.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7c1e9b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:=====================================================>(198 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+\n",
      "|state|airport_count|\n",
      "+-----+-------------+\n",
      "|   TX|           24|\n",
      "|   CA|           22|\n",
      "|   AK|           19|\n",
      "|   FL|           17|\n",
      "|   MI|           15|\n",
      "|   NY|           14|\n",
      "|   CO|           10|\n",
      "|   MN|            8|\n",
      "|   PA|            8|\n",
      "|   ND|            8|\n",
      "|   NC|            8|\n",
      "|   WI|            8|\n",
      "|   MT|            8|\n",
      "|   LA|            7|\n",
      "|   IL|            7|\n",
      "|   GA|            7|\n",
      "|   VA|            7|\n",
      "|   WY|            6|\n",
      "|   ID|            6|\n",
      "|   AL|            5|\n",
      "+-----+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, lit, col\n",
    "\n",
    "airport_codes_catalog. \\\n",
    "    groupBy(\"state\"). \\\n",
    "    agg(count(lit(1)).alias('airport_count')). \\\n",
    "    orderBy(col('airport_count').desc()). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4109dc77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b46c0912",
   "metadata": {},
   "source": [
    "## Creating Partitioned Tables\n",
    "\n",
    "We can also create partitioned tables as part of Spark Metastore Tables.\n",
    "\n",
    "* There are some challenges in creating partitioned tables directly using `spark.catalog.createTable`.\n",
    "* But if the directories are similar to partitioned tables with data, we should be able to create partitioned tables.\n",
    "* Let us create partitioned table for `orders` by `order_month`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58869f09",
   "metadata": {},
   "source": [
    "## Saving as Partitioned Tables\n",
    "\n",
    "We can also create partitioned tables while using `saveAsTable` function to write data from Dataframe into a metastore table.\n",
    "\n",
    "* Let us create partitioned table for `orders` by `order_month`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4764db0",
   "metadata": {},
   "source": [
    "Creating Temp Views\n",
    "So far we spoke about permanent metastore tables. Now let us understand how to create temporary views using a Data Frame.\n",
    "\n",
    "We can create temporary view for a Data Frame using createTempView or createOrReplaceTempView.\n",
    "createOrReplaceTempView will replace existing view, if it already exists.\n",
    "While tables in Metastore are permanent, views are temporary.\n",
    "Once the application exits, temporary views will be deleted or flushed out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9f5178",
   "metadata": {},
   "source": [
    "## Using Spark SQL\n",
    "\n",
    "Let us understand how we can use Spark SQL to process data in Metastore Tables and Temporary Views.\n",
    "\n",
    "* Once tables in metastore or temporary views are created, we can run queries against the tables or temporary views to perform all standard transformations.\n",
    "* We will create metastore tables for orders and order_items data sets. We will also create temporary view for products data set.\n",
    "* We will create metastore tables using `spark.sql` by passing `CREATE TABLE` statements as strings.\n",
    "* Using Spark SQL, we will join metastore tables and temporary view in the same query."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
