{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Spark 3 using CLIs\n",
    "\n",
    "Let us validate Spark 3.1.2 with Scala, Python as well as SQL using CLIs. We will also make sure that Spark 3.1.2 is integrated with Hive by running some queries against the databases and tables that are created using Hive earlier.\n",
    "\n",
    "* Validate Spark using Scala by running `/opt/spark3/bin/spark-shell --master yarn --conf spark.ui.port=0`.\n",
    "* Validate Spark using Python by running `/opt/spark3/bin/pyspark --master yarn --conf spark.ui.port=0`. \n",
    "* Make sure to export PYSPARK_PYTHON to point to Python 3 if the default version is 2.7.\n",
    "\n",
    "```shell\n",
    "export PYSPARK_PYTHON=python3\n",
    "/opt/spark3/bin/pyspark --master yarn --conf spark.ui.port=0\n",
    "```\n",
    "\n",
    "* You can update **.profile** with `export PYSPARK_PYTHON=python3` so that we don't need to export in the session while launching `pyspark`.\n",
    "* Validate Spark SQL by running `/opt/spark3/bin/spark-sql --master yarn --conf spark.ui.port=0`.\n",
    "* You can also validate whether you can access Hive Metastore tables and databases using Spark.\n",
    "\n",
    "```python\n",
    "spark.sql('SHOW databases').show()\n",
    "spark.sql('USE retail_db')\n",
    "\n",
    "spark.sql('SHOW tables').show()\n",
    "spark.sql('SELECT * FROM orders').show()\n",
    "spark.sql('SELECT count(1) FROM orders').show()\n",
    "\n",
    "exit()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\r\n",
      "Setting default log level to \"WARN\".\r\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\r\n"
     ]
    }
   ],
   "source": [
    "!spark-sql --master yarn  --conf spark.ui.port=0  --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark-sql> show databases;\n",
    "default\n",
    "retail_db\n",
    "Time taken: 2.971 seconds, Fetched 2 row(s)\n",
    "\n",
    "    \n",
    "spark-sql> USE retail_db;\n",
    "Time taken: 0.043 seconds\n",
    "\n",
    "    \n",
    "spark-sql> SHOW tables;\n",
    "retail_db\torders\tfalse\n",
    "Time taken: 0.141 seconds, Fetched 1 row(s)\n",
    "\n",
    "\n",
    "    \n",
    "spark-sql> SELECT * FROM orders LIMIT 10;\n",
    "1\t2013-07-25 00:00:00.0\t11599\tCLOSED\n",
    "2\t2013-07-25 00:00:00.0\t256\tPENDING_PAYMENT\n",
    "3\t2013-07-25 00:00:00.0\t12111\tCOMPLETE\n",
    "4\t2013-07-25 00:00:00.0\t8827\tCLOSED\n",
    "5\t2013-07-25 00:00:00.0\t11318\tCOMPLETE\n",
    "6\t2013-07-25 00:00:00.0\t7130\tCOMPLETE\n",
    "7\t2013-07-25 00:00:00.0\t4530\tCOMPLETE\n",
    "8\t2013-07-25 00:00:00.0\t2911\tPROCESSING\n",
    "9\t2013-07-25 00:00:00.0\t5657\tPENDING_PAYMENT\n",
    "10\t2013-07-25 00:00:00.0\t5648\tPENDING_PAYMENT\n",
    "Time taken: 24.034 seconds, Fetched 10 row(s)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "spark-sql> SELECT count(1) FROM orders;\n",
    "68883\n",
    "Time taken: 11.83 seconds, Fetched 1 row(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
